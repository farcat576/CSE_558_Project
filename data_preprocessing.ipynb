{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Show dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('initial_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(frac = 0.2, random_state=123)\n",
    "df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sample size: {len(df)} rows')\n",
    "print(df.head())\n",
    "\n",
    "# Check the size of the DataFrame\n",
    "num_rows, num_cols = df.shape\n",
    "total_elements = num_rows * num_cols\n",
    "print(f\"Initial dataset size: {num_rows} rows, {num_cols} columns, {total_elements} total elements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for noise in the data Using EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis to identify and visualize noise in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to analyze.\n",
    "    \"\"\"\n",
    "    # Display basic information about the dataset\n",
    "    print(\"Dataset Information:\")\n",
    "    print(df.info())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing Values per Column:\")\n",
    "    print(missing_values)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Heatmap of Missing Values')\n",
    "    plt.show()\n",
    "    print(\"The heatmap above shows the distribution of missing values across the dataset. Columns with more missing values will have more yellow lines.\")\n",
    "\n",
    "    # Visualize the distribution of numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        print(f\"The histogram above shows the distribution of '{col}'. Skewness or irregularities may indicate noise.\")\n",
    "\n",
    "        # Boxplot to detect outliers\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.show()\n",
    "        print(f\"The boxplot above for '{col}' helps identify outliers. Points outside the whiskers are potential outliers.\")\n",
    "\n",
    "        # Outlier Detection using IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if IQR == 0:\n",
    "            print(f\"No variation in '{col}'; skipping outlier detection.\\n\")\n",
    "            continue\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        num_outliers = outliers.shape[0]\n",
    "        print(f\"Number of outliers in '{col}': {num_outliers}\\n\")\n",
    "\n",
    "    # Visualize correlations between numerical variables\n",
    "    if len(numeric_cols) >= 2:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "        plt.title('Correlation Matrix of Numerical Variables')\n",
    "        plt.show()\n",
    "        print(\"The correlation matrix above shows the correlation coefficients between numerical variables. High correlations may indicate multicollinearity.\")\n",
    "\n",
    "    # Check for duplicates\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "    if num_duplicates > 0:\n",
    "        print(\"There are duplicate rows in the dataset, indicating data redundancy or duplication.\\n\")\n",
    "    else:\n",
    "        print(\"There are no duplicate rows in the dataset.\\n\")\n",
    "\n",
    "    # Analyze categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        num_unique_values = df[col].nunique()\n",
    "        print(f\"Categorical Variable '{col}' has {num_unique_values} unique values.\")\n",
    "        if num_unique_values < 20:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"Unique values in '{col}': {unique_values}\\n\")\n",
    "        else:\n",
    "            print(f\"'{col}' has many unique values; we should consider checking for inconsistencies or encoding if necessary.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the dataset by removing rows with missing values and handling duplicates and outliers.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # Remove rows with any missing values\n",
    "    print(\"Removing rows with missing values...\")\n",
    "    initial_row_count = df_cleaned.shape[0]\n",
    "    df_cleaned = df_cleaned.dropna()\n",
    "    final_row_count = df_cleaned.shape[0]\n",
    "    rows_removed = initial_row_count - final_row_count\n",
    "    print(f\"Removed {rows_removed} rows due to missing values.\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    print(\"\\nRemoving duplicate rows...\")\n",
    "    initial_row_count = df_cleaned.shape[0]\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    final_row_count = df_cleaned.shape[0]\n",
    "    duplicates_removed = initial_row_count - final_row_count\n",
    "    print(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "\n",
    "    # Handle outliers in numerical columns by removing them\n",
    "    print(\"\\nHandling outliers...\")\n",
    "    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df_cleaned[col].quantile(0.25)\n",
    "        Q3 = df_cleaned[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if IQR == 0:\n",
    "            print(f\"No variation in '{col}'; skipping outlier handling.\")\n",
    "            continue\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Remove rows with outliers\n",
    "        outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)]\n",
    "        num_outliers = outliers.shape[0]\n",
    "        df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
    "        print(f\"Removed {num_outliers} outliers from '{col}'.\")\n",
    "\n",
    "    # Handle inconsistencies in categorical variables\n",
    "    print(\"\\nHandling inconsistencies in categorical variables...\")\n",
    "    categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        # Convert to lowercase and strip whitespaces\n",
    "        df_cleaned[col] = df_cleaned[col].str.lower().str.strip()\n",
    "        print(f\"Standardized text in '{col}' by converting to lowercase and stripping whitespaces.\")\n",
    "\n",
    "    print(\"\\nData cleaning completed.\")\n",
    "\n",
    "    # Print unique values of each column to verify\n",
    "    print(\"\\nUnique values in each column after cleaning:\")\n",
    "    for col in df_cleaned.columns:\n",
    "        num_unique_values = df_cleaned[col].nunique()\n",
    "        print(f\"Column '{col}' has {num_unique_values} unique values.\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df_cleaned.shape\n",
    "total_elements = x * y\n",
    "total_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset to a new CSV file\n",
    "output_file_path = 'cleaned_data.csv'\n",
    "df_cleaned.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_eda(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows having the substring nan in any column\n",
    "print(df_cleaned[df_cleaned.isin(['nan']).any(axis=1)])\n",
    "# Empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaned!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print col datatypes\n",
    "df_cleaned.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
